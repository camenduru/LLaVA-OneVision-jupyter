{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/LLaVA-OneVision-jupyter/blob/main/LLaVA_OneVision_Qwen2_7b_si_jupyter.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone -b totoro4 https://github.com/camenduru/ComfyUI /content/TotoroUI\n",
        "!git clone -b totoro4 https://github.com/camenduru/ComfyUI-LLaVA-OneVision /content/TotoroUI/custom_nodes/TotoroUI-LLaVA-OneVision\n",
        "%cd /content/TotoroUI\n",
        "\n",
        "!pip install -q torchsde einops diffusers accelerate xformers==0.0.27 gradio==4.40.0 bitsandbytes\n",
        "\n",
        "%cd /content/TotoroUI\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import nodes\n",
        "from nodes import NODE_CLASS_MAPPINGS\n",
        "from nodes import load_custom_node\n",
        "from totoro_extras import nodes_custom_sampler\n",
        "from totoro_extras import nodes_flux\n",
        "from totoro import model_management\n",
        "import gradio as gr\n",
        "\n",
        "load_custom_node(\"/content/TotoroUI/custom_nodes/TotoroUI-LLaVA-OneVision\")\n",
        "\n",
        "DownloadAndLoadLLaVAOneVisionModel = NODE_CLASS_MAPPINGS[\"DownloadAndLoadLLaVAOneVisionModel\"]()\n",
        "LLaVA_OneVision_Run = NODE_CLASS_MAPPINGS[\"LLaVA_OneVision_Run\"]()\n",
        "LoadImage =  NODE_CLASS_MAPPINGS[\"LoadImage\"]()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    llava_model = DownloadAndLoadLLaVAOneVisionModel.loadmodel(\"lmms-lab/llava-onevision-qwen2-7b-si\", \"cuda\", \"nf4\", \"sdpa\")[0]\n",
        "\n",
        "def closestNumber(n, m):\n",
        "    q = int(n / m)\n",
        "    n1 = m * q\n",
        "    if (n * m) > 0:\n",
        "        n2 = m * (q + 1)\n",
        "    else:\n",
        "        n2 = m * (q - 1)\n",
        "    if abs(n - n1) < abs(n - n2):\n",
        "        return n1\n",
        "    return n2\n",
        "\n",
        "@torch.inference_mode()\n",
        "def tag(image, tag_prompt):\n",
        "    image = LoadImage.load_image(image)[0]\n",
        "    text = LLaVA_OneVision_Run.run(image, llava_model, tag_prompt, 4096, True)[0]\n",
        "    return text\n",
        "\n",
        "with gr.Blocks(analytics_enabled=False) as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            image = gr.Image(type='filepath')\n",
        "        with gr.Column():\n",
        "            tag_prompt = gr.Textbox(lines=3, interactive=True, value=\"prompt style\", label=\"Tag Prompt\")\n",
        "            tag_button = gr.Button(\"Tag\")\n",
        "            positive_prompt = gr.Textbox(lines=3, interactive=True, value=\"\", label=\"Image Prompt\")\n",
        "\n",
        "    tag_button.click(fn=tag, inputs=[image, tag_prompt], outputs=positive_prompt)\n",
        "\n",
        "demo.queue().launch(inline=False, share=True, debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
